{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents <a name='top'></a>\n",
    "\n",
    "- [Load Modules and Set Notebook Properties](#modules)\n",
    "- [Define Path and Load Data](#load)\n",
    "- [Inspect Data](#inspect)\n",
    "- [Prepare](#prepare)\n",
    "- [Scale Values](#scale)\n",
    "- [Create Different ANN Models](#create)\n",
    "- [Find the Best Model](#evaluate)\n",
    "- [Evaluate and Choose Models](#evaluate)\n",
    "- [Predict](#predict)\n",
    "- [Prepare Submission](#submit)\n",
    "\n",
    "[go to end](#end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Modules and Set Notebook Properties <a name='modules'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, StandardScaler, MinMaxScaler, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Path and Load Data  <a name='load'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = 'raw_data_source'\n",
    "OUTPUT_PATH = 'outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_features = pd.read_csv(os.path.join(INPUT_PATH, 'covid_training_set_features.csv'))\n",
    "training_set_labels = pd.read_csv(os.path.join(INPUT_PATH, 'covid_training_set_labels.csv'))\n",
    "test_set_features = pd.read_csv(os.path.join(INPUT_PATH, 'covid_test_set_features.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Data <a name='inspect'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Background <a name=\"databackground\"></a>\n",
    "\n",
    "In this exercise, we will take a look at vaccination, a key public health measure used to fight infectious diseases. Vaccines provide immunization for individuals, and enough immunization in a community can further reduce the spread of diseases through \"herd immunity\".\n",
    "\n",
    "A phone survey asked respondents whether they had received the H1N1 and seasonal flu vaccines, in conjunction with questions about themselves. These additional questions covered their social, economic, and demographic background, opinions on risks of illness and vaccine effectiveness, and behaviors towards mitigating transmission. A better understanding of how these characteristics are associated with personal vaccination patterns can provide guidance for future public health efforts.\n",
    "\n",
    "The goal is to predict how likely individuals are to receive their H1N1 and seasonal flu vaccines. Specifically, we will be predicting two probabilities: one for h1n1_vaccine and one for seasonal_vaccine. Each row in the dataset represents one person who responded to the National 2009 H1N1 Flu Survey.\n",
    "\n",
    "The dataset is taken from the competetion page in [DrivenData](https://www.drivendata.org/competitions/66/flu-shot-learning/page/210/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data <a name=\"dataprep\"></a>\n",
    "\n",
    "[back to top](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(df):\n",
    "    \n",
    "    cols_to_process =  ['h1n1_concern', 'h1n1_knowledge',\n",
    "                        'opinion_h1n1_vacc_effective', 'opinion_h1n1_risk',\n",
    "                        'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective',\n",
    "                        'opinion_seas_risk', 'opinion_seas_sick_from_vacc', 'age_group',\n",
    "                        'education', 'race', 'sex', 'income_poverty', 'marital_status',\n",
    "                        'rent_or_own', 'employment_status', 'hhs_geo_region', 'census_msa',\n",
    "                        'household_adults', 'household_children', 'employment_industry',\n",
    "                        'employment_occupation']\n",
    "    \n",
    "    for i in cols_to_process:\n",
    "        df[i] = [f'{i}_' + str(x)  for x in df[i]]\n",
    "        \n",
    "    concat_list = []\n",
    "    for i in cols_to_process:\n",
    "        concat_list.append(pd.get_dummies(df[i]))\n",
    "        \n",
    "    one_hot_encoded = pd.concat(concat_list, axis=1)\n",
    "    df = df.drop(columns=cols_to_process)\n",
    "    df_concatenated = pd.concat([df, one_hot_encoded], axis=1)\n",
    "        \n",
    "    return df_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = process_features(training_set_features).iloc[:,1:].fillna(0)\n",
    "X_test = process_features(test_set_features).iloc[:,1:].fillna(0)\n",
    "y_h1n1 = training_set_labels['h1n1_vaccine']\n",
    "y_seasonal = training_set_labels['seasonal_vaccine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26707, 157), (26708, 157))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Values <a name='scale'></a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert explanation on why the fitting of the scaler should only be done on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_values(X_train, X_test, scaler='standard'):\n",
    "    \n",
    "    scaler_dict = {'standard': StandardScaler(), \n",
    "                    'minmax': MinMaxScaler(), \n",
    "                    'normal': Normalizer()}\n",
    "    if scaler is None:\n",
    "        return X_train, X_test\n",
    "    elif scaler not in scaler_dict.keys():\n",
    "        raise ValueError(\"Enter a valid value for scaler! Choose between 'standard', 'minmax', 'normal' or None.\")\n",
    "    else:\n",
    "        scl = scaler_dict[scaler]\n",
    "        X_train = scl.fit_transform(X_train)\n",
    "        X_test = scl.transform(X_test) \n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Different ANN Models <a name='create'></a> \n",
    "\n",
    "[back to top](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ann(X_train, y_train, epochs, batch_size, verbose):\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeper_ann(X_train, y_train, epochs, batch_size, verbose):\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wider_ann(X_train, y_train, epochs, batch_size, verbose):\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wider_and_deeper_ann(X_train, y_train, epochs, batch_size, verbose):\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_wrapper(X, y, model, test_size=0.3, scaler=None, epochs=5, batch_size=10, verbose=1):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=109) # split\n",
    "    X_train_, X_test_ = scale_values(X_train, X_test, scaler=scaler)  # scale\n",
    "    trained_model_h1n1 = model(X_train_, y_train, epochs, batch_size, verbose)  # train\n",
    "    probability = trained_model_h1n1.predict_proba(X_test_)  # predict and get probability score\n",
    "    \n",
    "    return roc_auc_score(y_test, probability.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the Best Model <a name='find'></a> \n",
    "\n",
    "[back to top](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18694/18694 [==============================] - 3s 159us/step - loss: 0.3984 - accuracy: 0.8317\n",
      "Epoch 2/10\n",
      "18694/18694 [==============================] - 3s 153us/step - loss: 0.3501 - accuracy: 0.8534\n",
      "Epoch 3/10\n",
      "18694/18694 [==============================] - 3s 154us/step - loss: 0.3360 - accuracy: 0.8598\n",
      "Epoch 4/10\n",
      "10670/18694 [================>.............] - ETA: 1s - loss: 0.3203 - accuracy: 0.8694"
     ]
    }
   ],
   "source": [
    "models = [simple_ann, deeper_ann, wider_ann, wider_and_deeper_ann]\n",
    "y_values = [y_h1n1, y_seasonal]\n",
    "results_dict = {}\n",
    "\n",
    "for y in y_values:\n",
    "    for model in models:\n",
    "        auc_score = model_wrapper(X, y, model, test_size=0.3, scaler='standard', epochs=10)\n",
    "        results_dict[f'{y.name} {model.__name__}'] = auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and Choose Models <a name='evaluate'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame.from_dict(results_dict, orient='index').reset_index()\n",
    "result_df.columns = ['index', 'auc_score']\n",
    "result_df['variable'] = [x.split(' ')[0] for x in result_df['index']]\n",
    "result_df['model'] = [x.split(' ')[1] for x in result_df['index']]\n",
    "result_df[['variable', 'model', 'auc_score']].pivot(index='variable', columns='model', values='auc_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For `h1n1_vaccine` use `wider_ann`\n",
    "- For `seasonal_vaccine` use `wider_ann`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict <a name='predict'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_test_ = scale_values(X, X_test, scaler='standard')\n",
    "seasonal_model = wider_ann(X_train_, y_seasonal, epochs=10, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_seasonal = seasonal_model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_test_ = scale_values(X, X_test, scaler='standard')\n",
    "h1n1_model = wider_ann(X_train_, y_h1n1, epochs=10, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_h1n1 = h1n1_model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Submission <a name='submit'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['respondent_id'] = test_set_features['respondent_id']\n",
    "submission['h1n1_vaccine'] = probability_h1n1\n",
    "submission['seasonal_vaccine'] = probability_seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = os.path.join(OUTPUT_PATH, 'sub1.csv')\n",
    "submission.to_csv(outpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--end--\n",
    "<a name=\"bottom\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
